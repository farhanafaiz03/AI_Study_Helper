# -*- coding: utf-8 -*-
"""Ai_Study-BUddy(Assistant).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yMY4sP_Ju6XNmfJHqaMhqZjIk_nTnfhS
"""

!pip install transformers pyttsx3 spacy PyPDF2
!python -m spacy download en_core_web_sm

!apt-get update && apt-get install -y espeak # Install espeak using apt-get

import pyttsx3
import PyPDF2
from transformers import T5ForConditionalGeneration, T5Tokenizer
import spacy
import torch
import random
import os


USE_TTS = False
if USE_TTS:
    tts_engine = pyttsx3.init()

def speak(text):
    """Optional text-to-speech"""
    if USE_TTS:
        tts_engine.say(text)
        tts_engine.runAndWait()

# MODEL
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


model_name = "t5-small"  # Faster than t5-base
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)


try:
    nlp = spacy.load("en_core_web_sm")
except:
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

# PDF TEXT EXTRACTION
def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        return ''.join([page.extract_text() or '' for page in reader.pages])

# summarizer
def summarize_text(text):
    chunks = [text[i:i+800] for i in range(0, len(text), 800)]
    summaries = []
    for chunk in chunks[:2]:  # Limit to 2 chunks max for speed
        input_text = "summarize: " + chunk
        inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True).to(device)
        summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        summaries.append(summary)
    final_summary = "\n".join(summaries)
    speak(final_summary)
    return final_summary

# flash card generator
def generate_flashcards(text):
    questions = []
    answers = []
    sentences = text.split('.')
    for sentence in sentences[:100]:  # Limit for faster performance
        doc = nlp(sentence.strip())
        for ent in doc.ents:
            if ent.label_ in ["PERSON", "ORG", "GPE"]:
                question = f"Who is {ent.text}?"
                if question not in questions:
                    questions.append(question)
                    answers.append(ent.text)

    flashcards = list(zip(questions, answers))
    speak(f"Generated {len(flashcards)} flashcards.")
    return "\n".join([f"Q: {q}\nA: {a}" for q, a in flashcards])


def answer_question(context, question):
    context_chunk = context[:1024]
    input_text = f"question: {question} context: {context_chunk}"
    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True).to(device)
    answer_ids = model.generate(inputs, max_length=100, num_beams=4, early_stopping=True)
    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)
    speak(answer)
    return answer


if __name__ == "__main__":
    pdf_path = input("Enter the path to your PDF file: ").strip()
    study_text = extract_text_from_pdf(pdf_path)

    while True:
        print("\nOptions:\n1. Summarize\n2. Ask a question\n3. Generate flashcards\n4. Exit")
        choice = input("Choose an option: ").strip()

        if choice == '1':
            summary = summarize_text(study_text)
            print("\nSummary:\n", summary)

        elif choice == '2':
            question = input("Enter your question: ")
            answer = answer_question(study_text, question)
            print("\nAnswer:\n", answer)

        elif choice == '3':
            flashcards = generate_flashcards(study_text)
            print("\nFlashcards:\n", flashcards)

        elif choice == '4':
            print("Goodbye!")
            break

        else:
            print("Invalid choice.")

